{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LT26iX8beKNi"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Task To Do:\n",
        "• Implement the softmax Function by completing the code or writing your own function.\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "D6_n97zWeVeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    \"\"\"\n",
        "    Compute the softmax of a 2D numpy array along the specified axis.\n",
        "    Parameters:\n",
        "    z (numpy.ndarray): Input array of shape (m, n) where m is the number of samples\n",
        "\n",
        "    and n is the number of classes.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Softmax probabilities of the same shape as input (m, n), where\n",
        "    each row sums to 1 and represents the probability distribution\n",
        "    over classes for a sample.\n",
        "\n",
        "    Notes:\n",
        "    - Applies a normalization trick to prevent numerical instability by subtracting\n",
        "    the max value in each row before exponentiation.\n",
        "    \"\"\"\n",
        "    # Normalize input to prevent numerical instability\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "_pH-Q4ZReY1S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Case\n",
        "def test_softmax():\n",
        "  \"\"\"\n",
        "  Perform basic assertion tests on the softmax function to validate its correctness.\n",
        "  Tests:\n",
        "  - Ensure that the output probabilities sum to 1 for each row.\n",
        "  - Ensure non-negative values (all probabilities should be >= 0).\n",
        "  - Test on edge cases (e.g., all zeros, very large or small values).\n",
        "  \"\"\"\n",
        "  # Test input\n",
        "  test_cases = [\n",
        "  (np.array([[0, 0, 0]]), \"All zeros\"),\n",
        "  (np.array([[1, 2, 3]]), \"Simple case\"),\n",
        "  (np.array([[1000, 1000, 1000]]), \"Large identical values\"),\n",
        "  (np.array([[-1000, -1000, -1000]]), \"Small identical values\"),\n",
        "  (np.array([[1, 0, -1]]), \"Mixed positive and negative\")\n",
        "  ]\n",
        "  for i, (z, description) in enumerate(test_cases):\n",
        "    print(f\"Test {i + 1}: {description}\")\n",
        "    result = softmax(z)\n",
        "    # Check that probabilities sum to 1\n",
        "    assert np.allclose(result.sum(axis=1), 1), f\"Failed: Probabilities do not sum to 1 in {description}\"\n",
        "    # Check non-negativity\n",
        "    assert np.all(result >= 0), f\"Failed: Negative probabilities in {description}\"\n",
        "    print(\"Passed.\")\n",
        "print(\"All tests passed for softmax function.\")\n",
        "test_softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx6U-_xueqf6",
        "outputId": "31684cfa-dd6b-4abe-da2e-e929f14e9fc3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed for softmax function.\n",
            "Test 1: All zeros\n",
            "Passed.\n",
            "Test 2: Simple case\n",
            "Passed.\n",
            "Test 3: Large identical values\n",
            "Passed.\n",
            "Test 4: Small identical values\n",
            "Passed.\n",
            "Test 5: Mixed positive and negative\n",
            "Passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Task To Do:\n",
        "• Implement the loss softmax Function by completing the code or writing your own function.\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "O4fATZ56fQPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Function for a Single Observation\n",
        "def loss_softmax(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Compute the cross-entropy loss for a single observation.\n",
        "  Parameters:\n",
        "  y_true (numpy.ndarray): True labels (one-hot encoded) of shape (c,).\n",
        "  y_pred (numpy.ndarray): Predicted probabilities of shape (c,).\n",
        "  Returns:\n",
        "  float: Cross-entropy loss for the observation.\n",
        "  \"\"\"\n",
        "  return -np.sum(y_true * np.log(y_pred + 1e-10)) # Add epsilon to prevent log(0)"
      ],
      "metadata": {
        "id": "kFYV0MlGfQ7X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Case\n",
        "def test_loss_softmax():\n",
        "  \"\"\"\n",
        "  Test the loss_softmax function using a known input and output.\n",
        "  \"\"\"\n",
        "  # Test Case 1: Perfect prediction\n",
        "  y_true = np.array([0, 1, 0]) # True label (one-hot encoded)\n",
        "  y_pred = np.array([0.1, 0.8, 0.1]) # Predicted probabilities\n",
        "  expected_loss = -np.log(0.8) # Expected loss for perfect prediction\n",
        "  assert np.isclose(loss_softmax(y_true, y_pred), expected_loss), \"Test Case 1 Failed\"\n",
        "  # Test Case 2: Incorrect prediction\n",
        "  y_true = np.array([1, 0, 0]) # True label (one-hot encoded)\n",
        "  y_pred = np.array([0.3, 0.4, 0.3]) # Predicted probabilities\n",
        "  expected_loss = -np.log(0.3) # Expected loss for incorrect prediction\n",
        "  assert np.isclose(loss_softmax(y_true, y_pred), expected_loss), \"Test Case 2 Failed\"\n",
        "  # Test Case 3: Edge case with near-zero probability\n",
        "  y_true = np.array([0, 1, 0]) # True label (one-hot encoded)\n",
        "  y_pred = np.array([0.01, 0.98, 0.01]) # Predicted probabilities\n",
        "  expected_loss = -np.log(0.98) # Expected loss for edge case\n",
        "  assert np.isclose(loss_softmax(y_true, y_pred), expected_loss), \"Test Case 3 Failed\"\n",
        "  print(\"All test cases passed!\")\n",
        "# Run the test\n",
        "test_loss_softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7n9rMVwfYtP",
        "outputId": "e6170a76-30c0-4e47-9f87-102bbace3fb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Task To Do:\n",
        "• Implement the cost softmax Function by completing the code or writing your own function.\n",
        "• Make sure you pass the test case."
      ],
      "metadata": {
        "id": "6XTpu8zCfj5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cost Function for Softmax (Average Loss)\n",
        "def cost_softmax(X, y, W, b):\n",
        "  \"\"\"\n",
        "  Compute the average cross-entropy cost over all samples.\n",
        "  Parameters:\n",
        "  X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "  y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "  W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "  b (numpy.ndarray): Bias vector of shape (c,).\n",
        "  Returns:\n",
        "  float: Average cross-entropy cost over all samples.\n",
        "  \"\"\"\n",
        "  n, d = X.shape\n",
        "  z = np.dot(X, W) + b\n",
        "  y_pred = softmax(z)\n",
        "  return -np.sum(y * np.log(y_pred + 1e-10)) / n"
      ],
      "metadata": {
        "id": "N063fhvSfleJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Case"
      ],
      "metadata": {
        "id": "N_ERRZeTjJ_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_cost_softmax():\n",
        "  \"\"\"\n",
        "  Test the cost_softmax function using a known input and output.\n",
        "  \"\"\"\n",
        "  # Test Case 1: Small dataset with perfect predictions\n",
        "  X = np.array([[1, 2], [2, 3], [3, 4]]) # Feature matrix (n=3, d=2)\n",
        "  y = np.array([[1, 0], [0, 1], [1, 0]]) # True labels (n=3, c=2, one-hot encoded)\n",
        "  W = np.array([[1, -1], [-1, 1]]) # Weight matrix (d=2, c=2)\n",
        "  b = np.array([0, 0]) # Bias vector (c=2)\n",
        "  z = np.dot(X, W) + b\n",
        "  y_pred = softmax(z) # Predicted probabilities\n",
        "  expected_cost = -np.sum(y * np.log(y_pred + 1e-10)) / X.shape[0] # Compute expected cost\n",
        "  assert np.isclose(cost_softmax(X, y, W, b), expected_cost), \"Test Case 1 Failed\"\n",
        "  # Test Case 2: All-zero weights and bias\n",
        "  X = np.array([[1, 0], [0, 1], [1, 1]]) # Feature matrix (n=3, d=2)\n",
        "  y = np.array([[1, 0], [0, 1], [1, 0]]) # True labels (n=3, c=2, one-hot encoded)\n",
        "  W = np.zeros((2, 2)) # Zero weight matrix\n",
        "  b = np.zeros(2) # Zero bias vector\n",
        "  z = np.dot(X, W) + b\n",
        "  y_pred = softmax(z) # Predicted probabilities (uniform distribution)\n",
        "  expected_cost = -np.sum(y * np.log(y_pred + 1e-10)) / X.shape[0] # Compute expected cost\n",
        "  assert np.isclose(cost_softmax(X, y, W, b), expected_cost), \"Test Case 2 Failed\"\n",
        "  print(\"All test cases passed!\")\n",
        "# Run the test\n",
        "test_cost_softmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDOPN18WjNUh",
        "outputId": "46380b75-cca8-4794-f3d9-5ff598518d19"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All test cases passed!\n"
          ]
        }
      ]
    }
  ]
}